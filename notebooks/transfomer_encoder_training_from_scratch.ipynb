{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BCZ_pWgkTvOO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import re\n",
        "import random\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_recall_fscore_support,\n",
        "    classification_report,\n",
        ")\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------\n",
        "# 1. Config\n",
        "# -----------------------\n",
        "\n",
        "CSV_PATH = \"/content/sample_data/merged.csv\"  # <- change if needed\n",
        "RANDOM_SEED = 42\n",
        "BATCH_SIZE = 32\n",
        "MAX_LEN = 64            # you can tune this\n",
        "EMBED_DIM = 128\n",
        "N_HEAD = 4\n",
        "NUM_LAYERS = 2\n",
        "FF_DIM = 256\n",
        "DROPOUT = 0.1\n",
        "NUM_EPOCHS = 10\n",
        "LR = 1e-3\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "random.seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(RANDOM_SEED)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKRq8wSVVRvE",
        "outputId": "e7987c16-73db-447d-9245-bf09649136fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------\n",
        "# 2. Load data\n",
        "# -----------------------\n",
        "\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "df['label'] = df['label'].str.upper()\n",
        "df = df[df['label'].str.lower() != 'label']\n",
        "df = df[df['label'].str.lower() != 'api_failure']\n",
        "# expecting columns: 'comment', 'label'\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ny88PY6OW24M",
        "outputId": "e3e582e6-5b27-42d0-ed89-c438ae510914"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text        label\n",
            "0                                      ÐÐÐ¦Ð˜Ð¡Ð¢ Ð¤ÐÐ¨Ð˜Ð¡Ð¢    OFFENSIVE\n",
            "1  Ð¥Ð°Ñ‡Ð¸ÐºÐ¸ Ð¿Ð¾Ð¿ÑƒÑ‚Ð°Ð»Ð¸ Ñ‡Ðµ Ñ‚Ð¾. ÐÐ°Ð´Ð¾ Ð¸Ð¼ Ð£Ñ€Ð¶Ð°Ñ€ 2.0 ÑƒÑÑ‚Ñ€Ð¾Ð¸Ñ‚ÑŒ  HATE_SPEECH\n",
            "2                                            ÐœÐ°Ð» ÑˆÑ‰Ñ    OFFENSIVE\n",
            "3  ÐÐµ Ñ‚Ð°Ñ‰Ð¸Ñ‚Ðµ ÑÑ‚Ñƒ Ñ…ÑƒÐµÑ‚Ñƒ ÑÑŽÐ´Ð°, Ð¿Ð¾Ð¶Ð°Ð»ÑƒÐ¹ÑÑ‚Ð°, ÑƒÐ¶Ðµ Ð² Ñ‚Ñ€...    OFFENSIVE\n",
            "4                              Ð‘Ð°Ñ€Ð±Ð°Ð³Ñ‹Ð»Ð° Ð°ÐºÐ¼Ð°ÐºÑ‚Ð°Ñ€Ð³Ð°!    OFFENSIVE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------\n",
        "# 3. EDA (label distribution, imbalance)\n",
        "# -----------------------\n",
        "\n",
        "label_counts = df[\"label\"].value_counts()\n",
        "print(\"\\nLabel counts:\")\n",
        "print(label_counts)\n",
        "\n",
        "print(\"\\nLabel proportions:\")\n",
        "print((label_counts / len(df)).round(3))\n",
        "\n",
        "# Imbalance ratio (max_count / min_count)\n",
        "imbalance_ratio = label_counts.max() / label_counts.min()\n",
        "print(f\"\\nImbalance ratio (max/min): {imbalance_ratio:.2f}\")\n",
        "\n",
        "# Comment length statistics\n",
        "df[\"text_len\"] = df[\"text\"].astype(str).apply(lambda x: len(x.split()))\n",
        "print(\"\\nText length stats (in tokens):\")\n",
        "print(df[\"text_len\"].describe())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nXFmM0AXM-i",
        "outputId": "540369a9-6296-4e5f-ce59-3995c224a340"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Label counts:\n",
            "label\n",
            "NORMAL         1052\n",
            "HATE_SPEECH     498\n",
            "OFFENSIVE       396\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Label proportions:\n",
            "label\n",
            "NORMAL         0.541\n",
            "HATE_SPEECH    0.256\n",
            "OFFENSIVE      0.203\n",
            "Name: count, dtype: float64\n",
            "\n",
            "Imbalance ratio (max/min): 2.66\n",
            "\n",
            "Text length stats (in tokens):\n",
            "count    1946.000000\n",
            "mean        9.843782\n",
            "std        11.683234\n",
            "min         1.000000\n",
            "25%         4.000000\n",
            "50%         7.000000\n",
            "75%        11.000000\n",
            "max       201.000000\n",
            "Name: text_len, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------\n",
        "# 4. Encode labels\n",
        "# -----------------------\n",
        "\n",
        "label2id = {label: idx for idx, label in enumerate(sorted(df[\"label\"].unique()))}\n",
        "id2label = {v: k for k, v in label2id.items()}\n",
        "print(\"\\nLabel mapping:\", label2id)\n",
        "\n",
        "df[\"label_id\"] = df[\"label\"].map(label2id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJrF4ScyXXFH",
        "outputId": "e9f993ed-a0a5-44d9-851b-8ba40772ddf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Label mapping: {'HATE_SPEECH': 0, 'NORMAL': 1, 'OFFENSIVE': 2}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------\n",
        "# 5. Train/val split\n",
        "# -----------------------\n",
        "train_df, val_df = train_test_split(\n",
        "    df,\n",
        "    test_size = 0.2,\n",
        "    random_state=RANDOM_SEED,\n",
        "    stratify=df[\"label_id\"],\n",
        "    )\n",
        "\n",
        "print(\"\\nTrain size:\", len(train_df), \"Val size:\", len(val_df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xqxxTYSXaXT",
        "outputId": "15b7a642-09e9-46ae-c87e-352b73ebaced"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train size: 1556 Val size: 390\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------\n",
        "# 6. Tokenization & Vocabulary\n",
        "# -----------------------\n",
        "\n",
        "def basic_tokenize(text: str):\n",
        "    # Simple whitespace tokenizer, lowercasing\n",
        "    # You can improve: remove extra spaces, keep punctuation as tokens, etc.\n",
        "    text = str(text).lower().strip()\n",
        "    # optional: replace multiple spaces\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    return text.split()\n",
        "\n",
        "# Build vocabulary on train set only\n",
        "special_tokens = [\"<pad>\", \"<unk>\", \"<cls>\"]\n",
        "PAD_TOKEN = \"<pad>\"\n",
        "UNK_TOKEN = \"<unk>\"\n",
        "CLS_TOKEN = \"<cls>\"\n",
        "\n",
        "# Collect tokens\n",
        "all_tokens = []\n",
        "for text in train_df[\"text\"]:\n",
        "    toks = basic_tokenize(text)\n",
        "    all_tokens.extend(toks)\n",
        "\n",
        "# Frequency threshold if you want to drop rare words\n",
        "token_freq = Counter(all_tokens)\n",
        "min_freq = 2  # you can tune this (or set to 1 to keep everything)\n",
        "\n",
        "vocab_tokens = [tok for tok, c in token_freq.items() if c >= min_freq]\n",
        "vocab = special_tokens + sorted(vocab_tokens)\n",
        "\n",
        "token2id = {tok: idx for idx, tok in enumerate(vocab)}\n",
        "id2token = {idx: tok for tok, idx in token2id.items()}\n",
        "\n",
        "PAD_IDX = token2id[PAD_TOKEN]\n",
        "UNK_IDX = token2id[UNK_TOKEN]\n",
        "CLS_IDX = token2id[CLS_TOKEN]\n",
        "\n",
        "print(\"\\nVocab size:\", len(vocab))\n",
        "\n",
        "def encode_text(text: str, max_len: int = MAX_LEN):\n",
        "    tokens = basic_tokenize(text)\n",
        "    # prepend CLS token\n",
        "    tokens = [CLS_TOKEN] + tokens\n",
        "\n",
        "    # convert to ids\n",
        "    ids = [token2id.get(tok, UNK_IDX) for tok in tokens]\n",
        "\n",
        "    # truncate\n",
        "    ids = ids[:max_len]\n",
        "\n",
        "    # pad\n",
        "    attention_mask = [1] * len(ids)\n",
        "    if len(ids) < max_len:\n",
        "        pad_len = max_len - len(ids)\n",
        "        ids = ids + [PAD_IDX] * pad_len\n",
        "        attention_mask = attention_mask + [0] * pad_len\n",
        "\n",
        "    return ids, attention_mask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSMJP4TrY13h",
        "outputId": "8a9ec4f0-f47c-46df-dbc3-10a26972ce79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Vocab size: 1814\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------\n",
        "# 7. Dataset & DataLoader\n",
        "# -----------------------\n",
        "\n",
        "class HateSpeechDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.texts = df[\"text\"].tolist()\n",
        "        self.labels = df[\"label_id\"].tolist()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        input_ids, attention_mask = encode_text(text, max_len=MAX_LEN)\n",
        "        input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
        "        attention_mask = torch.tensor(attention_mask, dtype=torch.long)\n",
        "        label = torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"labels\": label\n",
        "        }\n",
        "\n",
        "train_dataset = HateSpeechDataset(train_df)\n",
        "val_dataset = HateSpeechDataset(val_df)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ],
      "metadata": {
        "id": "zeNfBpUnY3ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------\n",
        "# 8. Transformer Encoder model\n",
        "# -----------------------\n",
        "\n",
        "class TransformerClassifier(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size,\n",
        "        num_classes,\n",
        "        d_model=EMBED_DIM,\n",
        "        nhead=N_HEAD,\n",
        "        num_layers=NUM_LAYERS,\n",
        "        dim_feedforward=FF_DIM,\n",
        "        dropout=0.1,\n",
        "        max_len=MAX_LEN,\n",
        "        pad_idx=PAD_IDX,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.pad_idx = pad_idx\n",
        "        self.max_len = max_len\n",
        "\n",
        "        self.token_embedding = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)\n",
        "        self.pos_embedding = nn.Embedding(max_len, d_model)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "            batch_first=True,  # easier: (batch, seq, dim)\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(\n",
        "            encoder_layer,\n",
        "            num_layers=num_layers\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(d_model, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        \"\"\"\n",
        "        input_ids: (batch, seq_len)\n",
        "        attention_mask: (batch, seq_len) with 1 for tokens, 0 for padding\n",
        "        \"\"\"\n",
        "        batch_size, seq_len = input_ids.shape\n",
        "\n",
        "        # token embeddings\n",
        "        tok_emb = self.token_embedding(input_ids)  # (batch, seq_len, d_model)\n",
        "\n",
        "        # positional embeddings\n",
        "        positions = torch.arange(0, seq_len, device=input_ids.device).unsqueeze(0)\n",
        "        pos_emb = self.pos_embedding(positions)  # (1, seq_len, d_model)\n",
        "\n",
        "        x = tok_emb + pos_emb\n",
        "\n",
        "        # src_key_padding_mask: (batch, seq_len) True where padding\n",
        "        if attention_mask is not None:\n",
        "            src_key_padding_mask = attention_mask == 0\n",
        "        else:\n",
        "            src_key_padding_mask = (input_ids == self.pad_idx)\n",
        "\n",
        "        x = self.transformer_encoder(\n",
        "            x,\n",
        "            src_key_padding_mask=src_key_padding_mask\n",
        "        )  # (batch, seq_len, d_model)\n",
        "\n",
        "        # Use CLS token representation (first token)\n",
        "        cls_out = x[:, 0, :]  # (batch, d_model)\n",
        "        cls_out = self.dropout(cls_out)\n",
        "        logits = self.fc(cls_out)  # (batch, num_classes)\n",
        "        return logits\n",
        "\n",
        "num_classes = len(label2id)\n",
        "model = TransformerClassifier(\n",
        "    vocab_size=len(vocab),\n",
        "    num_classes=num_classes,\n",
        "    d_model=EMBED_DIM,\n",
        "    nhead=N_HEAD,\n",
        "    num_layers=NUM_LAYERS,\n",
        "    dim_feedforward=FF_DIM,\n",
        "    dropout=DROPOUT,\n",
        "    max_len=MAX_LEN,\n",
        "    pad_idx=PAD_IDX,\n",
        ").to(device)\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHPb1P6eY7Ha",
        "outputId": "cfc7a376-6315-4023-a24b-f358ead61b66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TransformerClassifier(\n",
            "  (token_embedding): Embedding(1814, 128, padding_idx=0)\n",
            "  (pos_embedding): Embedding(64, 128)\n",
            "  (transformer_encoder): TransformerEncoder(\n",
            "    (layers): ModuleList(\n",
            "      (0-1): 2 x TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
            "        )\n",
            "        (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (linear2): Linear(in_features=256, out_features=128, bias=True)\n",
            "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout1): Dropout(p=0.1, inplace=False)\n",
            "        (dropout2): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (fc): Linear(in_features=128, out_features=3, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------\n",
        "# 9. Handle class imbalance (weights)\n",
        "# -----------------------\n",
        "\n",
        "train_label_counts = train_df[\"label_id\"].value_counts().sort_index().values\n",
        "# Inverse frequency\n",
        "class_weights = 1.0 / train_label_counts\n",
        "class_weights = class_weights / class_weights.sum() * len(class_weights)  # normalize-ish\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "\n",
        "print(\"\\nClass weights:\", class_weights)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dY7g6AveY_71",
        "outputId": "51d957f9-1d1c-4596-c5a8-4218e54ac4eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Class weights: tensor([1.0994, 0.5203, 1.3803])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------\n",
        "# 10. Training & Evaluation loops\n",
        "# -----------------------\n",
        "\n",
        "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for batch in dataloader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(input_ids, attention_mask=attention_mask)\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * input_ids.size(0)\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader.dataset)\n",
        "    return avg_loss\n",
        "\n",
        "def evaluate(model, dataloader, device):\n",
        "    model.eval()\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "    total_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            logits = model(input_ids, attention_mask=attention_mask)\n",
        "            loss = criterion(logits, labels)\n",
        "\n",
        "            total_loss += loss.item() * input_ids.size(0)\n",
        "\n",
        "            preds = torch.argmax(logits, dim=-1)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader.dataset)\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        all_labels, all_preds, average=\"macro\", zero_division=0\n",
        "    )\n",
        "    report = classification_report(\n",
        "        all_labels, all_preds,\n",
        "        target_names=[id2label[i] for i in range(num_classes)],\n",
        "        zero_division=0\n",
        "    )\n",
        "    return avg_loss, acc, precision, recall, f1, report\n"
      ],
      "metadata": {
        "id": "lWMJ8HaZZGlx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------\n",
        "# 11. Train loop\n",
        "# -----------------------\n",
        "\n",
        "best_val_f1 = 0.0\n",
        "for epoch in range(1, NUM_EPOCHS + 1):\n",
        "    train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
        "    val_loss, val_acc, val_prec, val_rec, val_f1, val_report = evaluate(model, val_loader, device)\n",
        "\n",
        "    print(f\"\\nEpoch {epoch}/{NUM_EPOCHS}\")\n",
        "    print(f\"  Train loss: {train_loss:.4f}\")\n",
        "    print(f\"  Val loss  : {val_loss:.4f}\")\n",
        "    print(f\"  Val Acc   : {val_acc:.4f}\")\n",
        "    print(f\"  Val Prec  : {val_prec:.4f}\")\n",
        "    print(f\"  Val Recall: {val_rec:.4f}\")\n",
        "    print(f\"  Val F1    : {val_f1:.4f}\")\n",
        "\n",
        "    # Save best model by F1\n",
        "    if val_f1 > best_val_f1:\n",
        "        best_val_f1 = val_f1\n",
        "        torch.save(model.state_dict(), \"best_transformer_hatespeech.pt\")\n",
        "        print(\"  ðŸ”¥ New best model saved!\")\n",
        "\n",
        "print(\"\\nFinal evaluation on validation set (best epoch metrics above):\")\n",
        "print(val_report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3D_tqWxJZKaF",
        "outputId": "52d96fbe-82ca-4c17-86ee-8b2b6b8f7d8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:515: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)\n",
            "  output = torch._nested_tensor_from_mask(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/10\n",
            "  Train loss: 1.0456\n",
            "  Val loss  : 0.9423\n",
            "  Val Acc   : 0.5949\n",
            "  Val Prec  : 0.7357\n",
            "  Val Recall: 0.5789\n",
            "  Val F1    : 0.5477\n",
            "  ðŸ”¥ New best model saved!\n",
            "\n",
            "Epoch 2/10\n",
            "  Train loss: 0.8058\n",
            "  Val loss  : 0.7603\n",
            "  Val Acc   : 0.6846\n",
            "  Val Prec  : 0.6448\n",
            "  Val Recall: 0.6527\n",
            "  Val F1    : 0.6419\n",
            "  ðŸ”¥ New best model saved!\n",
            "\n",
            "Epoch 3/10\n",
            "  Train loss: 0.6060\n",
            "  Val loss  : 0.7793\n",
            "  Val Acc   : 0.6718\n",
            "  Val Prec  : 0.6466\n",
            "  Val Recall: 0.6694\n",
            "  Val F1    : 0.6450\n",
            "  ðŸ”¥ New best model saved!\n",
            "\n",
            "Epoch 4/10\n",
            "  Train loss: 0.4451\n",
            "  Val loss  : 0.9257\n",
            "  Val Acc   : 0.6231\n",
            "  Val Prec  : 0.6320\n",
            "  Val Recall: 0.6403\n",
            "  Val F1    : 0.6046\n",
            "\n",
            "Epoch 5/10\n",
            "  Train loss: 0.3429\n",
            "  Val loss  : 1.0012\n",
            "  Val Acc   : 0.7282\n",
            "  Val Prec  : 0.6926\n",
            "  Val Recall: 0.6674\n",
            "  Val F1    : 0.6771\n",
            "  ðŸ”¥ New best model saved!\n",
            "\n",
            "Epoch 6/10\n",
            "  Train loss: 0.2635\n",
            "  Val loss  : 1.0555\n",
            "  Val Acc   : 0.7026\n",
            "  Val Prec  : 0.6635\n",
            "  Val Recall: 0.6823\n",
            "  Val F1    : 0.6710\n",
            "\n",
            "Epoch 7/10\n",
            "  Train loss: 0.2119\n",
            "  Val loss  : 1.2234\n",
            "  Val Acc   : 0.7436\n",
            "  Val Prec  : 0.7134\n",
            "  Val Recall: 0.6812\n",
            "  Val F1    : 0.6940\n",
            "  ðŸ”¥ New best model saved!\n",
            "\n",
            "Epoch 8/10\n",
            "  Train loss: 0.1866\n",
            "  Val loss  : 1.2941\n",
            "  Val Acc   : 0.7103\n",
            "  Val Prec  : 0.6999\n",
            "  Val Recall: 0.6817\n",
            "  Val F1    : 0.6756\n",
            "\n",
            "Epoch 9/10\n",
            "  Train loss: 0.1906\n",
            "  Val loss  : 1.2001\n",
            "  Val Acc   : 0.7103\n",
            "  Val Prec  : 0.6700\n",
            "  Val Recall: 0.6606\n",
            "  Val F1    : 0.6563\n",
            "\n",
            "Epoch 10/10\n",
            "  Train loss: 0.1425\n",
            "  Val loss  : 1.3489\n",
            "  Val Acc   : 0.6949\n",
            "  Val Prec  : 0.6655\n",
            "  Val Recall: 0.6775\n",
            "  Val F1    : 0.6603\n",
            "\n",
            "Final evaluation on validation set (best epoch metrics above):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " HATE_SPEECH       0.56      0.82      0.67       100\n",
            "      NORMAL       0.85      0.71      0.77       211\n",
            "   OFFENSIVE       0.59      0.51      0.54        79\n",
            "\n",
            "    accuracy                           0.69       390\n",
            "   macro avg       0.67      0.68      0.66       390\n",
            "weighted avg       0.72      0.69      0.70       390\n",
            "\n"
          ]
        }
      ]
    }
  ]
}